{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92cb6e6-a6a9-49e8-be04-a41c819e4a54",
   "metadata": {},
   "source": [
    "# Your first RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7caa13-e543-42d0-91b9-0901bf7c1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9227d7e-9e65-4052-a384-7bb230776cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 819c2adf5ce6: 100% ▕██████████████████▏ 669 MB                         \u001b[K\n",
      "pulling c71d239df917: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling b837481ff855: 100% ▕██████████████████▏   16 B                         \u001b[K\n",
      "pulling 38badd946f91: 100% ▕██████████████████▏  408 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling ff82381e2bea: 100% ▕██████████████████▏ 4.1 GB                         \u001b[K\n",
      "pulling 43070e2d4e53: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling 491dfa501e59: 100% ▕██████████████████▏  801 B                         \u001b[K\n",
      "pulling ed11eda7790d: 100% ▕██████████████████▏   30 B                         \u001b[K\n",
      "pulling 42347cd80dc8: 100% ▕██████████████████▏  485 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Obtain your embedding & LLM models (execute here or go to terminal)\n",
    "!ollama pull mxbai-embed-large\n",
    "!ollama pull mistral  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6402fab7-c192-49be-920c-e765cbe52b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        ID              SIZE      MODIFIED               \n",
      "mistral:latest              f974a74358d6    4.1 GB    Less than a second ago    \n",
      "mxbai-embed-large:latest    468836162de7    669 MB    Less than a second ago    \n"
     ]
    }
   ],
   "source": [
    "# Check model availability \n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f33c1b2-fe9f-4bc8-9000-7834ca52df69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='mistral', created_at='2025-05-02T15:47:33.237839Z', done=True, done_reason='stop', total_duration=5486703250, load_duration=12943084, prompt_eval_count=13, prompt_eval_duration=3082628125, eval_count=49, eval_duration=2390351875, message=Message(role='assistant', content=\" Why don't Data Scientists play hide and seek with their data?\\n\\nBecause they always use find() function to search for it!\\n\\n(Apologies for the lame joke, I'll work on my humor.)\", images=None, tool_calls=None))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = ollama.chat(model=\"mistral\", \n",
    "            messages=[ {\"role\": \"user\", \"content\": \"Tell me a joke about Data Scientists\"}]\n",
    "           )\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04dbe35b-9d84-47bf-9977-fb4bd7bd2305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why don't Data Scientists play hide and seek with their data?\n",
      "\n",
      "Because they always use find() function to search for it!\n",
      "\n",
      "(Apologies for the lame joke, I'll work on my humor.)\n"
     ]
    }
   ],
   "source": [
    "print(res['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73cb663-5674-4b00-9dff-ac48aabb12c7",
   "metadata": {},
   "source": [
    "# Set up your first RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc7cbe3-e05c-4b97-afd5-2ad98b60ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import DefaultEmbeddingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9a96c67-239e-42be-b7b0-e826308cc956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Setup ChromaDB ===\n",
    "chroma_client = chromadb.Client()\n",
    "collection    = chroma_client.get_or_create_collection(name=\"rag-docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "804ebf50-a317-4acf-b10a-d36ebe454b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: Load and Embed Documents ===\n",
    "def embed_text(text):\n",
    "    response = ollama.embed(model=\"mxbai-embed-large\", input=text)\n",
    "    return response[\"embeddings\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c286e704-7107-4742-98c2-3c26eb48a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample docs (could also read from files)\n",
    "documents = [\n",
    "    \"Jurgen Klopp was born in Germany in 1974. He has been a successful coach in the UK\",\n",
    "    \"You can contact Sky customer support through the help portal or live chat.\",\n",
    "    \"An apple a day keeps the doctor away\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d11fef32-f46b-460e-9228-71579128bcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  6.75it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, doc in tqdm(enumerate(documents)):\n",
    "    embedding = embed_text(doc)\n",
    "    collection.add(\n",
    "        documents=[doc],\n",
    "        embeddings=[embedding],\n",
    "        ids=[f\"doc-{i}\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fe82b71-1774-4cf6-b025-a53755827383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['doc-0', 'doc-1', 'doc-2']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Jurgen Klopp was born in Germany in 1974. He has been a successful coach in the UK',\n",
       "   'You can contact Sky customer support through the help portal or live chat.',\n",
       "   'An apple a day keeps the doctor away']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[None, None, None]],\n",
       " 'distances': [[0.3875125050544739, 1.4115504026412964, 1.4422112703323364]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Step 3: Accept User Query and Retrieve Relevant Docs ===\n",
    "query = \"Who was Jurgen Klopp?\"\n",
    "\n",
    "query_embedding = embed_text(query)\n",
    "results = collection.query(query_embeddings=[query_embedding], n_results=3)\n",
    "\n",
    "results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5364da15-dc59-4824-bcd2-e0da60c6d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = results[\"documents\"][0]\n",
    "context = \"\\n\".join(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "742e4870-f481-4b86-95f1-1110cfae31ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question using only the context below.\n",
      "\n",
      "Context:\n",
      "Jurgen Klopp was born in Germany in 1974. He has been a successful coach in the UK\n",
      "You can contact Sky customer support through the help portal or live chat.\n",
      "An apple a day keeps the doctor away\n",
      "\n",
      "Question: Who was Jurgen Klopp?\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# === Step 4: Run RAG Prompt through Ollama LLM ===\n",
    "prompt = f\"\"\"Answer the question using only the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42fc099c-c848-479f-b78a-e9f98a02c4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Answer:\n",
      "  Jurgen Klopp is a German born individual who has been a successful coach in the UK.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model=\"mistral\", messages=[\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "])\n",
    "\n",
    "print(\"RAG Answer:\\n\", response[\"message\"][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai101)",
   "language": "python",
   "name": "genai101-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
