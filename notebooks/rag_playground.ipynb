{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92cb6e6-a6a9-49e8-be04-a41c819e4a54",
   "metadata": {},
   "source": [
    "# Your first RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7caa13-e543-42d0-91b9-0901bf7c1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import rich\n",
    "\n",
    "import ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9227d7e-9e65-4052-a384-7bb230776cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 819c2adf5ce6: 100% ▕██████████████████▏ 669 MB                         \u001b[K\n",
      "pulling c71d239df917: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling b837481ff855: 100% ▕██████████████████▏   16 B                         \u001b[K\n",
      "pulling 38badd946f91: 100% ▕██████████████████▏  408 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling ff82381e2bea: 100% ▕██████████████████▏ 4.1 GB                         \u001b[K\n",
      "pulling 43070e2d4e53: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling 491dfa501e59: 100% ▕██████████████████▏  801 B                         \u001b[K\n",
      "pulling ed11eda7790d: 100% ▕██████████████████▏   30 B                         \u001b[K\n",
      "pulling 42347cd80dc8: 100% ▕██████████████████▏  485 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Obtain your embedding & LLM models (execute here or go to terminal)\n",
    "!ollama pull mxbai-embed-large\n",
    "!ollama pull mistral  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6402fab7-c192-49be-920c-e765cbe52b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        ID              SIZE      MODIFIED               \n",
      "mistral:latest              f974a74358d6    4.1 GB    Less than a second ago    \n",
      "mxbai-embed-large:latest    468836162de7    669 MB    Less than a second ago    \n"
     ]
    }
   ],
   "source": [
    "# Check model availability \n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f33c1b2-fe9f-4bc8-9000-7834ca52df69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'mistral'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-05-09T21:02:08.839524Z'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">done</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">done_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">total_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5827962542</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">load_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16437000</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">prompt_eval_count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">prompt_eval_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">466928375</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">eval_count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">eval_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5343878125</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Message</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">' Why don\\'t Data Scientists go out in the rain?\\n\\nBecause they have too many features and only </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">one label for \"wet\"!\\n\\n(But in all seriousness, Data Scientists love working with large datasets and finding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">patterns, even if it means getting drenched in data!)'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">images</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'mistral'\u001b[0m,\n",
       "    \u001b[33mcreated_at\u001b[0m=\u001b[32m'2025-05-09T21:02:08.839524Z'\u001b[0m,\n",
       "    \u001b[33mdone\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mdone_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "    \u001b[33mtotal_duration\u001b[0m=\u001b[1;36m5827962542\u001b[0m,\n",
       "    \u001b[33mload_duration\u001b[0m=\u001b[1;36m16437000\u001b[0m,\n",
       "    \u001b[33mprompt_eval_count\u001b[0m=\u001b[1;36m12\u001b[0m,\n",
       "    \u001b[33mprompt_eval_duration\u001b[0m=\u001b[1;36m466928375\u001b[0m,\n",
       "    \u001b[33meval_count\u001b[0m=\u001b[1;36m64\u001b[0m,\n",
       "    \u001b[33meval_duration\u001b[0m=\u001b[1;36m5343878125\u001b[0m,\n",
       "    \u001b[33mmessage\u001b[0m=\u001b[1;35mMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m' Why don\\'t Data Scientists go out in the rain?\\n\\nBecause they have too many features and only \u001b[0m\n",
       "\u001b[32mone label for \"wet\"!\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mBut in all seriousness, Data Scientists love working with large datasets and finding \u001b[0m\n",
       "\u001b[32mpatterns, even if it means getting drenched in data!\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[33mimages\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First run \n",
    "res = ollama.chat(model=\"mistral\", \n",
    "            messages=[ {\"role\": \"user\", \"content\": \"Tell me a joke about Data Science\"}]\n",
    "           )\n",
    "rich.print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73cb663-5674-4b00-9dff-ac48aabb12c7",
   "metadata": {},
   "source": [
    "## Set up your first RAG pipeline\n",
    "\n",
    "The main components of a naive RAG pipeline are: \n",
    "\n",
    "1. Index your documents (embed) \n",
    "2. Build a retriever\n",
    "3. Augment the capabilities of the LLM \n",
    "4. Synthesis of the answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acc7cbe3-e05c-4b97-afd5-2ad98b60ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import DefaultEmbeddingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a96c67-239e-42be-b7b0-e826308cc956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Setup ChromaDB ===\n",
    "chroma_client = chromadb.Client()\n",
    "collection    = chroma_client.get_or_create_collection(name=\"rag-docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "804ebf50-a317-4acf-b10a-d36ebe454b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: Load and Embed Documents ===\n",
    "def embed_text(text):\n",
    "    response = ollama.embed(model=\"mxbai-embed-large\", input=text)\n",
    "    return response[\"embeddings\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c286e704-7107-4742-98c2-3c26eb48a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample docs (could also read from files)\n",
    "documents = [\n",
    "    \"Jurgen Klopp was born in Germany in 1974. He has been a successful coach in the UK\",\n",
    "    \"You can contact Sky customer support through the help portal or live chat.\",\n",
    "    \"An apple a day keeps the doctor away\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d11fef32-f46b-460e-9228-71579128bcbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7003e851a4c944bba9a8b9af56028f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, doc in tqdm(enumerate(documents)):\n",
    "    embedding = embed_text(doc)\n",
    "    collection.add( documents=[doc],\n",
    "                    embeddings=[embedding],\n",
    "                    ids=[f\"doc-{i}\"]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fe82b71-1774-4cf6-b025-a53755827383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ids'</span>: <span style=\"font-weight: bold\">[[</span><span style=\"color: #008000; text-decoration-color: #008000\">'doc-0'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'doc-1'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'doc-2'</span><span style=\"font-weight: bold\">]]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'embeddings'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'documents'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Jurgen Klopp was born in Germany in 1974. He has been a successful coach in the UK'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'You can contact Sky customer support through the help portal or live chat.'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'An apple a day keeps the doctor away'</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'uris'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'included'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'metadatas'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'documents'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'distances'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'data'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'metadatas'</span>: <span style=\"font-weight: bold\">[[</span><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">]]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'distances'</span>: <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3585015535354614</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.367884874343872</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.476802945137024</span><span style=\"font-weight: bold\">]]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'doc-0'\u001b[0m, \u001b[32m'doc-1'\u001b[0m, \u001b[32m'doc-2'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'embeddings'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'documents'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m[\u001b[0m\n",
       "            \u001b[32m'Jurgen Klopp was born in Germany in 1974. He has been a successful coach in the UK'\u001b[0m,\n",
       "            \u001b[32m'You can contact Sky customer support through the help portal or live chat.'\u001b[0m,\n",
       "            \u001b[32m'An apple a day keeps the doctor away'\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'uris'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'included'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'metadatas'\u001b[0m, \u001b[32m'documents'\u001b[0m, \u001b[32m'distances'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'data'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'metadatas'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[3;35mNone\u001b[0m, \u001b[3;35mNone\u001b[0m, \u001b[3;35mNone\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'distances'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.3585015535354614\u001b[0m, \u001b[1;36m1.367884874343872\u001b[0m, \u001b[1;36m1.476802945137024\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Step 3: Accept User Query and Retrieve Relevant Docs ===\n",
    "query = \"What team did Jurgen Klopp coach?\"\n",
    "\n",
    "query_embedding = embed_text(query)\n",
    "results = collection.query(query_embeddings=[query_embedding], n_results=3)\n",
    "\n",
    "rich.print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5364da15-dc59-4824-bcd2-e0da60c6d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = results[\"documents\"][0]\n",
    "context = \"\\n\".join(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "742e4870-f481-4b86-95f1-1110cfae31ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You're a personal assistant. Your task is to answer questions using only the provided context. \n",
       "If you can not explicitly extract the answer from the context, your answer must be I cannot help with that. \n",
       "\n",
       "Context: <span style=\"font-weight: bold\">{</span>context<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "Question: <span style=\"font-weight: bold\">{</span>query<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "Answer:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "You're a personal assistant. Your task is to answer questions using only the provided context. \n",
       "If you can not explicitly extract the answer from the context, your answer must be I cannot help with that. \n",
       "\n",
       "Context: \u001b[1m{\u001b[0mcontext\u001b[1m}\u001b[0m\n",
       "\n",
       "Question: \u001b[1m{\u001b[0mquery\u001b[1m}\u001b[0m\n",
       "\n",
       "Answer:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Step 4: Run RAG Prompt through Ollama LLM ===\n",
    "answer_prompt = \"\"\"You're a personal assistant. Your task is to answer questions using only the provided context. \n",
    "If you can not explicitly extract the answer from the context, your answer must be I cannot help with that. \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "rich.print(answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3f93216-8a80-4f6d-b595-b053cfc40e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What team did Jurgen Klopp coach?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "857ec487-e9d8-4a46-bff4-6512d8dc9410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'mistral'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-05-09T21:02:13.913544Z'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">done</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">done_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">total_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3809865000</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">load_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6716375</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">prompt_eval_count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">119</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">prompt_eval_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1555983417</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">eval_count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">eval_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2246663250</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Message</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">' I cannot help with that, as the provided context does not mention which team Jurgen Klopp coached</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in the UK.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">images</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'mistral'\u001b[0m,\n",
       "    \u001b[33mcreated_at\u001b[0m=\u001b[32m'2025-05-09T21:02:13.913544Z'\u001b[0m,\n",
       "    \u001b[33mdone\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mdone_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "    \u001b[33mtotal_duration\u001b[0m=\u001b[1;36m3809865000\u001b[0m,\n",
       "    \u001b[33mload_duration\u001b[0m=\u001b[1;36m6716375\u001b[0m,\n",
       "    \u001b[33mprompt_eval_count\u001b[0m=\u001b[1;36m119\u001b[0m,\n",
       "    \u001b[33mprompt_eval_duration\u001b[0m=\u001b[1;36m1555983417\u001b[0m,\n",
       "    \u001b[33meval_count\u001b[0m=\u001b[1;36m26\u001b[0m,\n",
       "    \u001b[33meval_duration\u001b[0m=\u001b[1;36m2246663250\u001b[0m,\n",
       "    \u001b[33mmessage\u001b[0m=\u001b[1;35mMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m' I cannot help with that, as the provided context does not mention which team Jurgen Klopp coached\u001b[0m\n",
       "\u001b[32min the UK.'\u001b[0m,\n",
       "        \u001b[33mimages\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = ollama.chat(model=\"mistral\", \n",
    "            messages=[ {\"role\": \"user\", \"content\": answer_prompt.format(context=context, query=query)}]\n",
    "           )\n",
    "rich.print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7b9b7-c58d-4469-9be2-24212f77a9f4",
   "metadata": {},
   "source": [
    "## Build a proper index \n",
    "\n",
    "1. Take large documents and chunk them if needed\n",
    "2. Add relevant metadata to the documents to enhance search\n",
    "3. Embed and add to the collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf82d744-0536-4e53-86ba-17739e90c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40,000 lines of Shakespeare from a variety of Shakespeare's plays\n",
    "with open('../data/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65c023c3-a6d6-4a4b-a166-6cf143f117d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The split method produced 56 chunks\n"
     ]
    }
   ],
   "source": [
    "# Naive splitter \n",
    "def chunk_text(text, chunk_size, overlap=20):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlap  # move back by `overlap` characters\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text( text, chunk_size=2000, overlap=200 )\n",
    "print( f'The split method produced {len(chunks)} chunks' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7ea5794-6766-4e3e-9a4c-ae9cd9866b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc7469fe61d44439023c6ad9369468c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shakespeare_collection = chroma_client.get_or_create_collection(name=\"shakespeare-chunks\")\n",
    "\n",
    "i=0\n",
    "for chunk in tqdm(chunks): \n",
    "    embedding = embed_text(chunk)\n",
    "    shakespeare_collection.add( documents=[chunk],\n",
    "                    embeddings=[embedding],\n",
    "                    ids=[f\"{i}\"]\n",
    "                    )\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6733d59-c6a3-481f-91d8-17c947acaca8",
   "metadata": {},
   "source": [
    "## Generate a valid set of questions so I can evaluate the Retrieval & Generation\n",
    "\n",
    "Idea is simple: by extracting the questions from passages:\n",
    "- I can keep track of the origin \n",
    "- I can set up an evaluation method for retrieval efficacy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a841ad3a-c0ea-44a2-97ce-3aa093eecf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> <span style=\"color: #008000; text-decoration-color: #008000\">\"SUMMARY_PROMPT\"</span> = You are an expert Shakespeare analyst. You will receive a chunk of one of his books, and your \n",
       "task is to summarise what is happening in the passage.\n",
       "Write a short summary capturing the most relevant information of the passage in less than <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> words. \n",
       "\n",
       "Chunk: <span style=\"font-weight: bold\">{</span>chunks<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "Answer: \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " \u001b[32m\"SUMMARY_PROMPT\"\u001b[0m = You are an expert Shakespeare analyst. You will receive a chunk of one of his books, and your \n",
       "task is to summarise what is happening in the passage.\n",
       "Write a short summary capturing the most relevant information of the passage in less than \u001b[1;36m100\u001b[0m words. \n",
       "\n",
       "Chunk: \u001b[1m{\u001b[0mchunks\u001b[1m}\u001b[0m\n",
       "\n",
       "Answer: \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> <span style=\"color: #008000; text-decoration-color: #008000\">\"QUESTION_PROMPT\"</span> = You are an expert Shakespeare analyst. You will receive a summary of a passage of one of his \n",
       "books. \n",
       "Your task is to generate ONE simple, short, fact-based question that can be answered with the provided text alone. \n",
       "\n",
       "You must not mention that you are extracting the question from a text, a passage or a chunk. Avoid \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"in this passage from xxx\"</span>, references to the name of the book, or similar self-references.\n",
       "\n",
       "Text: <span style=\"font-weight: bold\">{</span>summary<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "Question: \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " \u001b[32m\"QUESTION_PROMPT\"\u001b[0m = You are an expert Shakespeare analyst. You will receive a summary of a passage of one of his \n",
       "books. \n",
       "Your task is to generate ONE simple, short, fact-based question that can be answered with the provided text alone. \n",
       "\n",
       "You must not mention that you are extracting the question from a text, a passage or a chunk. Avoid \n",
       "\u001b[32m\"in this passage from xxx\"\u001b[0m, references to the name of the book, or similar self-references.\n",
       "\n",
       "Text: \u001b[1m{\u001b[0msummary\u001b[1m}\u001b[0m\n",
       "\n",
       "Question: \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary_prompt = \"\"\"You are an expert Shakespeare analyst. You will receive a chunk of one of his books, and your task is to summarise what is happening in the passage.\n",
    "Write a short summary capturing the most relevant information of the passage in less than 100 words. \n",
    "\n",
    "Chunk: {chunks}\n",
    "\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "question_prompt = \"\"\"You are an expert Shakespeare analyst. You will receive a summary of a passage of one of his books. \n",
    "Your task is to generate ONE simple, short, fact-based question that can be answered with the provided text alone. \n",
    "\n",
    "You must not mention that you are extracting the question from a text, a passage or a chunk.\n",
    "\n",
    "Text: {summary}\n",
    "\n",
    "Question: \n",
    "\"\"\"\n",
    "\n",
    "rich.print(f' \"SUMMARY_PROMPT\" = {summary_prompt}')\n",
    "rich.print('------------')\n",
    "rich.print(f' \"QUESTION_PROMPT\" = {question_prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfbf93b2-27ae-4cd5-a8cb-7a9336fd8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question(chunks, chunk_id, verbose=False): \n",
    "\n",
    "    # Extract a summary from the provided passage\n",
    "    summary = ollama.chat(model=\"mistral\", messages=[\n",
    "    {\"role\": \"user\", \"content\": summary_prompt.format(chunks=chunks[chunk_id])}\n",
    "    ])\n",
    "\n",
    "    if verbose:\n",
    "        rich.print( f'Summary from chunk {chunk_id}: {summary[\"message\"][\"content\"]}')\n",
    "\n",
    "    # Extract question from the generated summary \n",
    "    question = ollama.chat(model=\"mistral\", messages=[\n",
    "    {\"role\": \"user\", \"content\": question_prompt.format(summary=summary[\"message\"][\"content\"])}\n",
    "    ])\n",
    "\n",
    "    question = question[\"message\"][\"content\"]\n",
    "    rich.print( f'Question from chunk {chunk_id}: {question}')\n",
    "\n",
    "    return question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d32f1445-a6c1-4a89-8583-ca5d5ed813ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Summary from chunk <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>:  The passage is from Coriolanus Act <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> Scene <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> and presents a dispute between the patricians \n",
       "<span style=\"font-weight: bold\">(</span>nobles<span style=\"font-weight: bold\">)</span> and plebeians <span style=\"font-weight: bold\">(</span>commoners<span style=\"font-weight: bold\">)</span> in Rome. The people are angry due to a food shortage, believing that the nobles \n",
       "are hoarding grain while they suffer. They have armed themselves and are marching towards the Capitol, intending to\n",
       "show their discontent with deeds, not just words. Menenius Agrippa, a noble who is well-liked by the people, tries \n",
       "to pacify them, stating that the nobles care for them like fathers. However, the first citizen expresses bitterness\n",
       "towards the nobles, accusing them of not caring for their welfare and making laws to oppress the poor. The scene \n",
       "ends with Menenius threatening to accuse the people if they do not change their actions. This tension between the \n",
       "classes sets the stage for further conflict throughout the play.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Summary from chunk \u001b[1;36m1\u001b[0m:  The passage is from Coriolanus Act \u001b[1;36m1\u001b[0m Scene \u001b[1;36m1\u001b[0m and presents a dispute between the patricians \n",
       "\u001b[1m(\u001b[0mnobles\u001b[1m)\u001b[0m and plebeians \u001b[1m(\u001b[0mcommoners\u001b[1m)\u001b[0m in Rome. The people are angry due to a food shortage, believing that the nobles \n",
       "are hoarding grain while they suffer. They have armed themselves and are marching towards the Capitol, intending to\n",
       "show their discontent with deeds, not just words. Menenius Agrippa, a noble who is well-liked by the people, tries \n",
       "to pacify them, stating that the nobles care for them like fathers. However, the first citizen expresses bitterness\n",
       "towards the nobles, accusing them of not caring for their welfare and making laws to oppress the poor. The scene \n",
       "ends with Menenius threatening to accuse the people if they do not change their actions. This tension between the \n",
       "classes sets the stage for further conflict throughout the play.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Question from chunk <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>:  What is the primary cause of tension between the patricians and plebeians in Coriolanus as \n",
       "shown at the beginning of the play?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Question from chunk \u001b[1;36m1\u001b[0m:  What is the primary cause of tension between the patricians and plebeians in Coriolanus as \n",
       "shown at the beginning of the play?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = extract_question( chunks, 1, verbose=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ad60e-ecf1-4076-a03c-e741efc68248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3696fb34104ae1958912fa7c1166c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Question from chunk <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>:  Who do the citizens in Coriolanus accuse of exploitation and indifference?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Question from chunk \u001b[1;36m1\u001b[0m:  Who do the citizens in Coriolanus accuse of exploitation and indifference?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Question from chunk <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>:  Who does Menenius accuse of hypocrisy in this dialogue?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Question from chunk \u001b[1;36m20\u001b[0m:  Who does Menenius accuse of hypocrisy in this dialogue?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Question from chunk <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>:  Who does Brutus advise against electing as a consul in <span style=\"color: #008000; text-decoration-color: #008000\">\"The Tragedy of Julius Caesar\"</span>?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Question from chunk \u001b[1;36m35\u001b[0m:  Who does Brutus advise against electing as a consul in \u001b[32m\"The Tragedy of Julius Caesar\"\u001b[0m?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Question from chunk <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42</span>:  Who orders the guards to seize Coriolanus in this passage?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Question from chunk \u001b[1;36m42\u001b[0m:  Who orders the guards to seize Coriolanus in this passage?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunk_ids = [1,20,35,42,48]\n",
    "questions = []\n",
    "\n",
    "for chunk_id in tqdm(chunk_ids):\n",
    "    question = extract_question( chunks, chunk_id )\n",
    "    questions.append(question)\n",
    "\n",
    "question_set = pd.DataFrame( {'chunk_id':chunk_ids, 'question': questions} )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc95f29e-3956-4d50-b52f-c53d57d050bf",
   "metadata": {},
   "source": [
    "# Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2a852-af31-4b5e-9b69-3ec8ffaf3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20 \n",
    "\n",
    "def find_position(lst, value):\n",
    "    try:\n",
    "        return lst.index(value)\n",
    "    except ValueError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122af50e-5902-40db-bb70-c6a1cb0276bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "found = []\n",
    "retrieved_chunks = []\n",
    "for idx in tqdm(question_set.index):\n",
    "    q   = question_set.at[idx, 'question'] \n",
    "    cid = question_set.at[idx, 'chunk_id'] \n",
    "\n",
    "    query_embedding = embed_text(q)\n",
    "    results         = shakespeare_collection.query(query_embeddings=[query_embedding], n_results=k)\n",
    "\n",
    "    retrieved_chunks.append( results['ids'][0] )\n",
    "    found.append( find_position( results['ids'][0], str(cid) ) )\n",
    "\n",
    "question_set['retrieval_rank']   = found \n",
    "question_set['retrieved_chunks'] = retrieved_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4c00de-d8ce-42c0-86b3-0d1d6f384263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval results \n",
    "question_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a54d08-8adf-44ae-b20f-1f2f7ecaa550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ranks     = range(0,k)\n",
    "precision = [ (question_set['retrieval_rank']<=rk).mean() for rk in ranks ]\n",
    "\n",
    "plt.title( 'Precision @k' )\n",
    "plt.plot( ranks, precision )\n",
    "plt.xlabel( 'k' )\n",
    "plt.ylabel( 'Precision' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6303bc-866f-4a99-8ac3-b84d242f140e",
   "metadata": {},
   "source": [
    "# Augment & Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668873e-2481-4ac2-a230-38be466d8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k     = 5\n",
    "synthesis = []\n",
    "\n",
    "for idx in tqdm(question_set.index):\n",
    "\n",
    "    question = question_set.at[idx, 'question']\n",
    "    context  = \"\\n\".join( [ chunks[int(cid)] for cid in question_set.at[idx, 'retrieved_chunks'][:top_k] ] )\n",
    "\n",
    "    response = ollama.chat(model=\"mistral\", messages=[\n",
    "    {\"role\": \"user\", \"content\": answer_prompt.format( context=context, query=question ) }\n",
    "])\n",
    "    answer = response[\"message\"][\"content\"]\n",
    "    rich.print( f' \"Query\": {question} \\n \"Answer\": {answer}' )\n",
    "    synthesis.append( answer )\n",
    "\n",
    "question_set['Responses'] = synthesis "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai101)",
   "language": "python",
   "name": "genai101-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
