{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92cb6e6-a6a9-49e8-be04-a41c819e4a54",
   "metadata": {},
   "source": [
    "# Your first RAG application (Retrieval-Augmented Generation)\n",
    "\n",
    "In this notebook, you'll build a simple **RAG (Retrieval-Augmented Generation)** pipeline â€” a powerful technique that combines the strength of large language models with external knowledge sources.\n",
    "\n",
    "RAG allows the model to **retrieve** relevant information from documents and **generate** accurate, **grounded** responses, instead of relying on the internal knowledge of the LLM.\n",
    "\n",
    "### ðŸ”  Why RAG?\n",
    "- Enhances model responses with up-to-date or domain-specific knowledge.\n",
    "- Reduces hallucinations by grounding answers in real data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906526d7-1101-4f64-9e07-27dfeea883c4",
   "metadata": {},
   "source": [
    "### Prepare all the necessary libs: Vector DB, and language model servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80186b0-f747-471a-95ac-bb7a991e4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vector DB. If run session needs to be re-initialised, do so and second install should only check it's already available. \n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee95a87-f81f-45ec-b405-163a74de4224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ollama to serve the models \n",
    "!curl https://ollama.ai/install.sh | sh\n",
    "!ollama --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771de64-274c-4f4c-a532-55fed07d9207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start ollama serve in the background using nohup and &\n",
    "!nohup ollama serve > /dev/null 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc2e79b-db63-4223-a936-2286b4e06f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain your embedding & LLM models (execute here or go to terminal)\n",
    "!ollama pull mxbai-embed-large\n",
    "!ollama pull mistral  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6402fab7-c192-49be-920c-e765cbe52b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model availability \n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bceeff0-99a8-4717-8150-dbd6084b7f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, install the python library\n",
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf5d988-496f-439b-bf2f-218a3ead9d57",
   "metadata": {},
   "source": [
    "### Import libs and start running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66338ca2-9603-49fa-a9d6-d95fd2bb8683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import rich\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f33c1b2-fe9f-4bc8-9000-7834ca52df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run \n",
    "res = ollama.chat(model=\"mistral\", \n",
    "            messages=[ {\"role\": \"user\", \"content\": \"Tell me a joke about Data Science\"}]\n",
    "           )\n",
    "rich.print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ada52-3c2e-46e7-a0e0-843ed1138460",
   "metadata": {},
   "source": [
    "# Build a naive RAG pipeline\n",
    "\n",
    "The main components of a naive RAG pipeline are: \n",
    "\n",
    "1. Will use an open-source embedding function\n",
    "2. ChromaDB to store all the generated documents and index\n",
    "3. Dense retrieval (semantic search) over the index to retrieve the most relevant content\n",
    "4. Augment the query with the retrieved context\n",
    "5. Generate an answer to the query, using the retrieved context  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a8c667-9a17-445f-b3da-2738a048cc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/marc-olm/genai101/main/docs/images/rag_diagram.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/marc-olm/genai101/main/docs/images/rag_diagram.png\"\n",
    "display(Image(url=url, width=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ef1ed-c31d-4210-a50e-881bdca29e79",
   "metadata": {},
   "source": [
    "## Set up your first RAG pipeline\n",
    "\n",
    "The index is created with all the (chunked) documents + the embedding function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d885ce9c-a2d9-4b2c-aea8-df55635341f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/marc-olm/genai101/main/docs/images/indexing.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/marc-olm/genai101/main/docs/images/indexing.png\"\n",
    "display(Image(url=url, width=700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7cbe3-e05c-4b97-afd5-2ad98b60ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a96c67-239e-42be-b7b0-e826308cc956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Setup ChromaDB ===\n",
    "chroma_client = chromadb.Client()\n",
    "collection    = chroma_client.get_or_create_collection(name=\"rag-docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ebf50-a317-4acf-b10a-d36ebe454b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: Load and Embed Documents ===\n",
    "def embed_text(text):\n",
    "    response = ollama.embed(model=\"mxbai-embed-large\", input=text)\n",
    "    return response[\"embeddings\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c286e704-7107-4742-98c2-3c26eb48a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample docs (could also read from files)\n",
    "documents = [\n",
    "    \"Jurgen Klopp was born in Germany in 1974. He has been a successful coach in the UK\",\n",
    "    \"You can contact Sky customer support through the help portal or live chat.\",\n",
    "    \"An apple a day keeps the doctor away\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11fef32-f46b-460e-9228-71579128bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in tqdm(enumerate(documents)):\n",
    "    embedding = embed_text(doc)\n",
    "    collection.add( documents=[doc],\n",
    "                    embeddings=[embedding],\n",
    "                    ids=[f\"doc-{i}\"]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe82b71-1774-4cf6-b025-a53755827383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: Accept User Query and Retrieve Relevant Docs ===\n",
    "query = \"What team did Jurgen Klopp coach?\"\n",
    "\n",
    "query_embedding = embed_text(query)\n",
    "results = collection.query(query_embeddings=[query_embedding], n_results=3)\n",
    "\n",
    "rich.print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364da15-dc59-4824-bcd2-e0da60c6d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = results[\"documents\"][0]\n",
    "context = \"\\n\".join(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e4870-f481-4b86-95f1-1110cfae31ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 4: Run RAG Prompt through Ollama LLM ===\n",
    "answer_prompt = \"\"\"You're a personal assistant. Your task is to answer questions using only the provided context. \n",
    "If you can not explicitly extract the answer from the context, your answer must be I cannot help with that. \n",
    "\n",
    "Your answer must be direct and contain no more than 150 words.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "<context start> \n",
    "{context}\n",
    "</context end>\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "rich.print(answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f93216-8a80-4f6d-b595-b053cfc40e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What team did Jurgen Klopp coach?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857ec487-e9d8-4a46-bff4-6512d8dc9410",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ollama.chat(model=\"mistral\", \n",
    "            messages=[ {\"role\": \"user\", \"content\": answer_prompt.format(context=context, query=query)}]\n",
    "           )\n",
    "rich.print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7b9b7-c58d-4469-9be2-24212f77a9f4",
   "metadata": {},
   "source": [
    "## Build a proper index \n",
    "\n",
    "1. Take large documents and chunk them if needed\n",
    "2. Add relevant metadata to the documents to enhance search\n",
    "3. Embed and add to the collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26e15ce-5983-4c40-be05-daa26b379a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading Shakespeare dataset...\")\n",
    "\n",
    "output_path = 'shakespeare.txt' \n",
    "url         = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "\n",
    "!curl -L -o {output_path} {url}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf82d744-0536-4e53-86ba-17739e90c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40,000 lines of Shakespeare from a variety of Shakespeare's plays\n",
    "with open(output_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c023c3-a6d6-4a4b-a166-6cf143f117d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive splitter \n",
    "def chunk_text(text, chunk_size, overlap=20):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlap  # move back by `overlap` characters\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text( text, chunk_size=2000, overlap=200 )\n",
    "print( f'The split method produced {len(chunks)} chunks' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea5794-6766-4e3e-9a4c-ae9cd9866b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_collection = chroma_client.get_or_create_collection(name=\"shakespeare-chunks\")\n",
    "\n",
    "i=0\n",
    "for chunk in tqdm(chunks): \n",
    "    embedding = embed_text(chunk)\n",
    "    shakespeare_collection.add( documents=[chunk],\n",
    "                    embeddings=[embedding],\n",
    "                    ids=[f\"{i}\"]\n",
    "                    )\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6733d59-c6a3-481f-91d8-17c947acaca8",
   "metadata": {},
   "source": [
    "## Generate a valid set of questions so I can evaluate the Retrieval & Generation\n",
    "\n",
    "Idea is simple: by extracting the questions from passages:\n",
    "- I can keep track of the origin \n",
    "- I can set up an evaluation method for retrieval efficacy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841ad3a-c0ea-44a2-97ce-3aa093eecf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = \"\"\"You are an expert Shakespeare analyst. You will receive a chunk of one of his books, and your task is to summarise what is happening in the passage.\n",
    "Write a short summary capturing the most relevant information of the passage in less than 100 words. \n",
    "\n",
    "Chunk: {chunks}\n",
    "\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "question_prompt = \"\"\"You are an expert Shakespeare analyst. You will receive a summary of a passage of one of his books. \n",
    "Your task is to generate ONE simple, short, fact-based question that can be answered with the provided text alone. \n",
    "\n",
    "You must not mention that you are extracting the question from a text, a passage or a chunk.\n",
    "\n",
    "Text: {summary}\n",
    "\n",
    "Question: \n",
    "\"\"\"\n",
    "\n",
    "rich.print(f' \"SUMMARY_PROMPT\" = {summary_prompt}')\n",
    "rich.print('------------')\n",
    "rich.print(f' \"QUESTION_PROMPT\" = {question_prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf93b2-27ae-4cd5-a8cb-7a9336fd8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question(chunks, chunk_id, verbose=False): \n",
    "\n",
    "    # Extract a summary from the provided passage\n",
    "    summary = ollama.chat(model=\"mistral\", messages=[\n",
    "    {\"role\": \"user\", \"content\": summary_prompt.format(chunks=chunks[chunk_id])}\n",
    "    ])\n",
    "\n",
    "    if verbose:\n",
    "        rich.print( f'Summary from chunk {chunk_id}: {summary[\"message\"][\"content\"]}')\n",
    "\n",
    "    # Extract question from the generated summary \n",
    "    question = ollama.chat(model=\"mistral\", messages=[\n",
    "    {\"role\": \"user\", \"content\": question_prompt.format(summary=summary[\"message\"][\"content\"])}\n",
    "    ])\n",
    "\n",
    "    question = question[\"message\"][\"content\"]\n",
    "    rich.print( f'Question from chunk {chunk_id}: {question}')\n",
    "\n",
    "    return question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f1445-a6c1-4a89-8583-ca5d5ed813ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = extract_question( chunks, 1, verbose=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ad60e-ecf1-4076-a03c-e741efc68248",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_ids = [1,20,35,42,48]\n",
    "questions = []\n",
    "\n",
    "for chunk_id in tqdm(chunk_ids):\n",
    "    question = extract_question( chunks, chunk_id )\n",
    "    questions.append(question)\n",
    "\n",
    "question_set = pd.DataFrame( {'chunk_id':chunk_ids, 'question': questions} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257249e-afbb-4101-9c08-fea8ee9c9bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated questions\n",
    "question_set.to_csv('rag_question_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f47f8-974e-43f5-8b6f-f6633dec6122",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_set_path = 'rag_question_set.csv'\n",
    "question_set_url  = 'https://raw.githubusercontent.com/marc-olm/genai101/main/notebooks/rag_question_set.csv'\n",
    "\n",
    "!curl -L -o {question_set_path} {question_set_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10a37b6-c279-42d6-b519-94c07b70bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_set = pd.read_csv(question_set_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc95f29e-3956-4d50-b52f-c53d57d050bf",
   "metadata": {},
   "source": [
    "# Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2a852-af31-4b5e-9b69-3ec8ffaf3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20 \n",
    "\n",
    "def find_position(lst, value):\n",
    "    try:\n",
    "        return lst.index(value)\n",
    "    except ValueError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122af50e-5902-40db-bb70-c6a1cb0276bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "found = []\n",
    "retrieved_chunks = []\n",
    "for idx in tqdm(question_set.index):\n",
    "    q   = question_set.at[idx, 'question'] \n",
    "    cid = question_set.at[idx, 'chunk_id'] \n",
    "\n",
    "    query_embedding = embed_text(q)\n",
    "    results         = shakespeare_collection.query(query_embeddings=[query_embedding], n_results=k)\n",
    "\n",
    "    retrieved_chunks.append( results['ids'][0] )\n",
    "    found.append( find_position( results['ids'][0], str(cid) ) )\n",
    "\n",
    "question_set['retrieval_rank']   = found \n",
    "question_set['retrieved_chunks'] = retrieved_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4c00de-d8ce-42c0-86b3-0d1d6f384263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval results \n",
    "question_set['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a54d08-8adf-44ae-b20f-1f2f7ecaa550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ranks     = range(0,k)\n",
    "precision = [ (question_set['retrieval_rank']<=rk).mean() for rk in ranks ]\n",
    "\n",
    "plt.title( 'Precision @k' )\n",
    "plt.plot( ranks, precision )\n",
    "plt.xlabel( 'k' )\n",
    "plt.ylabel( 'Precision' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6303bc-866f-4a99-8ac3-b84d242f140e",
   "metadata": {},
   "source": [
    "# Augment & Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e467a1d-c2a2-48c5-bed0-bddc8cb270a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668873e-2481-4ac2-a230-38be466d8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k     = 5\n",
    "synthesis = []\n",
    "\n",
    "for idx in tqdm(question_set.index):\n",
    "\n",
    "    question = question_set.at[idx, 'question']\n",
    "    context  = \"\\n\".join( [ chunks[int(cid)] for cid in question_set.at[idx, 'retrieved_chunks'][:top_k] ] )\n",
    "\n",
    "    response = ollama.chat(model=\"mistral\", messages=[\n",
    "    {\"role\": \"user\", \"content\": answer_prompt.format( context=context, query=question ) }\n",
    "])\n",
    "    answer = response[\"message\"][\"content\"]\n",
    "    rich.print( f' \"Query\": {question} \\n \"Answer\": {answer}' )\n",
    "    synthesis.append( answer )\n",
    "\n",
    "question_set['Responses'] = synthesis "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai101)",
   "language": "python",
   "name": "genai101-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
