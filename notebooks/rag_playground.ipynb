{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92cb6e6-a6a9-49e8-be04-a41c819e4a54",
   "metadata": {},
   "source": [
    "# Your first RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7caa13-e543-42d0-91b9-0901bf7c1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import ollama \n",
    "import rich\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9227d7e-9e65-4052-a384-7bb230776cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain your embedding & LLM models (execute here or go to terminal)\n",
    "!ollama pull mxbai-embed-large\n",
    "!ollama pull mistral  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6402fab7-c192-49be-920c-e765cbe52b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model availability \n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f33c1b2-fe9f-4bc8-9000-7834ca52df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run \n",
    "res = ollama.chat(model=\"mistral\", \n",
    "            messages=[ {\"role\": \"user\", \"content\": \"Tell me a joke about Data Science\"}]\n",
    "           )\n",
    "rich.print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73cb663-5674-4b00-9dff-ac48aabb12c7",
   "metadata": {},
   "source": [
    "## Set up your first RAG pipeline\n",
    "\n",
    "The main components of a naive RAG pipeline are: \n",
    "\n",
    "1. Index your documents (embed) \n",
    "2. Build a retriever\n",
    "3. Augment the capabilities of the LLM \n",
    "4. Synthesis of the answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7cbe3-e05c-4b97-afd5-2ad98b60ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import DefaultEmbeddingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a96c67-239e-42be-b7b0-e826308cc956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Setup ChromaDB ===\n",
    "chroma_client = chromadb.Client()\n",
    "collection    = chroma_client.get_or_create_collection(name=\"rag-docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ebf50-a317-4acf-b10a-d36ebe454b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: Load and Embed Documents ===\n",
    "def embed_text(text):\n",
    "    response = ollama.embed(model=\"mxbai-embed-large\", input=text)\n",
    "    return response[\"embeddings\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c286e704-7107-4742-98c2-3c26eb48a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample docs (could also read from files)\n",
    "documents = [\n",
    "    \"Jurgen Klopp was born in Germany in 1974. He has been a successful coach in the UK\",\n",
    "    \"You can contact Sky customer support through the help portal or live chat.\",\n",
    "    \"An apple a day keeps the doctor away\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11fef32-f46b-460e-9228-71579128bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in tqdm(enumerate(documents)):\n",
    "    embedding = embed_text(doc)\n",
    "    collection.add( documents=[doc],\n",
    "                    embeddings=[embedding],\n",
    "                    ids=[f\"doc-{i}\"]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe82b71-1774-4cf6-b025-a53755827383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: Accept User Query and Retrieve Relevant Docs ===\n",
    "query = \"What team did Jurgen Klopp coach?\"\n",
    "\n",
    "query_embedding = embed_text(query)\n",
    "results = collection.query(query_embeddings=[query_embedding], n_results=3)\n",
    "\n",
    "rich.print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364da15-dc59-4824-bcd2-e0da60c6d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = results[\"documents\"][0]\n",
    "context = \"\\n\".join(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e4870-f481-4b86-95f1-1110cfae31ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 4: Run RAG Prompt through Ollama LLM ===\n",
    "answer_prompt = \"\"\"You're a personal assistant. Your task is to answer questions using only the provided context. \n",
    "If you can not explicitly extract the answer from the context, your answer must be I cannot help with that. \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "rich.print(answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f93216-8a80-4f6d-b595-b053cfc40e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What team did Jurgen Klopp coach?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857ec487-e9d8-4a46-bff4-6512d8dc9410",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ollama.chat(model=\"mistral\", \n",
    "            messages=[ {\"role\": \"user\", \"content\": answer_prompt.format(context=context, query=query)}]\n",
    "           )\n",
    "rich.print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7b9b7-c58d-4469-9be2-24212f77a9f4",
   "metadata": {},
   "source": [
    "## Build a proper index \n",
    "\n",
    "1. Take large documents and chunk them if needed\n",
    "2. Add relevant metadata to the documents to enhance search\n",
    "3. Embed and add to the collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf82d744-0536-4e53-86ba-17739e90c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40,000 lines of Shakespeare from a variety of Shakespeare's plays\n",
    "with open('../data/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c023c3-a6d6-4a4b-a166-6cf143f117d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive splitter \n",
    "def chunk_text(text, chunk_size, overlap=20):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlap  # move back by `overlap` characters\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text( text, chunk_size=2000, overlap=200 )\n",
    "print( f'The split method produced {len(chunks)} chunks' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea5794-6766-4e3e-9a4c-ae9cd9866b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_collection = chroma_client.get_or_create_collection(name=\"shakespeare-chunks\")\n",
    "\n",
    "i=0\n",
    "for chunk in tqdm(chunks): \n",
    "    embedding = embed_text(chunk)\n",
    "    shakespeare_collection.add( documents=[chunk],\n",
    "                    embeddings=[embedding],\n",
    "                    ids=[f\"{i}\"]\n",
    "                    )\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6733d59-c6a3-481f-91d8-17c947acaca8",
   "metadata": {},
   "source": [
    "## Generate a valid set of questions so I can evaluate the Retrieval & Generation\n",
    "\n",
    "Idea is simple: by extracting the questions from passages:\n",
    "- I can keep track of the origin \n",
    "- I can set up an evaluation method for retrieval efficacy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841ad3a-c0ea-44a2-97ce-3aa093eecf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = \"\"\"You are an expert Shakespeare analyst. You will receive a chunk of one of his books, and your task is to summarise what is happening in the passage.\n",
    "Write a short summary capturing the most relevant information of the passage in less than 100 words. \n",
    "\n",
    "Chunk: {chunks}\n",
    "\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "question_prompt = \"\"\"You are an expert Shakespeare analyst. You will receive a summary of a passage of one of his books. \n",
    "Your task is to generate ONE simple, short, fact-based question that can be answered with the provided text alone. \n",
    "\n",
    "Do not include self-references to the text, such as: 'name of the book', 'according to the provided text', 'based on the provided passage' or similar self-references. \n",
    "\n",
    "Information: {summary}\n",
    "\n",
    "Question: \n",
    "\"\"\"\n",
    "\n",
    "rich.print(f' \"SUMMARY_PROMPT\" = {summary_prompt}')\n",
    "rich.print('------------')\n",
    "rich.print(f' \"QUESTION_PROMPT\" = {question_prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf93b2-27ae-4cd5-a8cb-7a9336fd8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question(chunks, chunk_id, verbose=False): \n",
    "\n",
    "    # Extract a summary from the provided passage\n",
    "    summary = ollama.chat(model=\"mistral\", messages=[\n",
    "    {\"role\": \"user\", \"content\": summary_prompt.format(chunks=chunks[chunk_id])}\n",
    "    ])\n",
    "\n",
    "    if verbose:\n",
    "        rich.print( f'Summary from chunk {chunk_id}: {summary[\"message\"][\"content\"]}')\n",
    "\n",
    "    # Extract question from the generated summary \n",
    "    question = ollama.chat(model=\"mistral\", messages=[\n",
    "    {\"role\": \"user\", \"content\": question_prompt.format(summary=summary[\"message\"][\"content\"])}\n",
    "    ])\n",
    "\n",
    "    question = question[\"message\"][\"content\"]\n",
    "    rich.print( f'Question from chunk {chunk_id}: {question}')\n",
    "\n",
    "    return question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f1445-a6c1-4a89-8583-ca5d5ed813ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = extract_question( chunks, 1, verbose=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ad60e-ecf1-4076-a03c-e741efc68248",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_ids = [1,20,35,42,48]\n",
    "questions = []\n",
    "\n",
    "for chunk_id in tqdm(chunk_ids):\n",
    "    question = extract_question( chunks, chunk_id )\n",
    "    questions.append(question)\n",
    "\n",
    "question_set = pd.DataFrame( {'chunk_id':chunk_ids, 'question': questions} )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc95f29e-3956-4d50-b52f-c53d57d050bf",
   "metadata": {},
   "source": [
    "# Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2a852-af31-4b5e-9b69-3ec8ffaf3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 20 \n",
    "\n",
    "def find_position(lst, value):\n",
    "    try:\n",
    "        return lst.index(value)\n",
    "    except ValueError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122af50e-5902-40db-bb70-c6a1cb0276bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "found = []\n",
    "retrieved_chunks = []\n",
    "for idx in tqdm(question_set.index):\n",
    "    q   = question_set.at[idx, 'question'] \n",
    "    cid = question_set.at[idx, 'chunk_id'] \n",
    "\n",
    "    query_embedding = embed_text(q)\n",
    "    results         = shakespeare_collection.query(query_embeddings=[query_embedding], n_results=k)\n",
    "\n",
    "    retrieved_chunks.append( results['ids'][0] )\n",
    "    found.append( find_position( results['ids'][0], str(cid) ) )\n",
    "\n",
    "question_set['retrieval_rank']   = found \n",
    "question_set['retrieved_chunks'] = retrieved_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4c00de-d8ce-42c0-86b3-0d1d6f384263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval results \n",
    "question_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a54d08-8adf-44ae-b20f-1f2f7ecaa550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ranks     = range(0,k)\n",
    "precision = [ (question_set['retrieval_rank']<=rk).mean() for rk in ranks ]\n",
    "\n",
    "plt.title( 'Precision @k' )\n",
    "plt.plot( ranks, precision )\n",
    "plt.xlabel( 'k' )\n",
    "plt.ylabel( 'Precision' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6303bc-866f-4a99-8ac3-b84d242f140e",
   "metadata": {},
   "source": [
    "# Augment & Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668873e-2481-4ac2-a230-38be466d8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k     = 5\n",
    "synthesis = []\n",
    "\n",
    "for idx in tqdm(question_set.index):\n",
    "\n",
    "    question = question_set.at[idx, 'question']\n",
    "    context  = \"\\n\".join( [ chunks[int(cid)] for cid in question_set.at[idx, 'retrieved_chunks'][:top_k] ] )\n",
    "\n",
    "    response = ollama.chat(model=\"mistral\", messages=[\n",
    "    {\"role\": \"user\", \"content\": answer_prompt.format( context=context, query=question ) }\n",
    "])\n",
    "    answer = response[\"message\"][\"content\"]\n",
    "    rich.print( f' \"Query\": {question} \\n \"Answer\": {answer}' )\n",
    "    synthesis.append( answer )\n",
    "\n",
    "question_set['Responses'] = synthesis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f484413-06bf-4426-9eae-2211d207fba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai101)",
   "language": "python",
   "name": "genai101-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
