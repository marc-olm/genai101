{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling 101 \n",
    "\n",
    "This notebook explores fundamental concepts in Natural Language Processing (NLP). These are the building blocks that allow machines to understand, process, and generate human language:\n",
    "\n",
    "ðŸ”¹ **Tokenization** breaks down raw text into smaller units like words, subwords, or characters.  \n",
    "ðŸ”¹ **Embeddings** map those tokens into dense vector spaces, enabling models to grasp semantic meaning and relationships between words.  \n",
    "ðŸ”¹ **Generation** train a model that predicts the next character in a sequence, forming the basis for simple text generation.\n",
    "\n",
    "inspired by https://karpathy.github.io/2015/05/21/rnn-effectiveness/ and source https://github.com/karpathy/char-rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import rich\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Shakespeare dataset...\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  1292k      0 --:--:-- --:--:-- --:--:-- 1292k\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading Shakespeare dataset...\")\n",
    "\n",
    "output_path = 'shakespeare.txt' \n",
    "url         = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "\n",
    "!curl -L -o {output_path} {url}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40,000 lines of Shakespeare from a variety of Shakespeare's plays\n",
    "with open(output_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'Length of dataset in characters: {len(text)}')\n",
    "print('\\nFirst 1000 characters:\\n')\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-Level Tokenization\n",
    "\n",
    "In character-level tokenization, each unique character in the text becomes a token. This is the simplest form of tokenization! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f'Vocabulary size (unique characters): {vocab_size}')\n",
    "print('\\nAll characters:', ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings from characters to integers and back\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Example: encode and decode some text\n",
    "example_text = \"Hello, World!\"\n",
    "encoded = [char_to_idx[ch] for ch in example_text]\n",
    "decoded = ''.join([idx_to_char[idx] for idx in encoded])\n",
    "\n",
    "print(char_to_idx)\n",
    "print(f'Original text: {example_text}')\n",
    "print(f'Encoded: {encoded}')\n",
    "print(f'Decoded: {decoded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Embeddings\n",
    "\n",
    "Now we'll create character embeddings. Each character will be represented by a vector in a higher-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for embeddings\n",
    "embedding_dim = 2  # Dimension of the embedding space\n",
    "\n",
    "# Create a random embedding table\n",
    "embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Example: get embeddings for a sequence of characters\n",
    "example_sequence = torch.tensor(encoded)  # Using the encoded text from above\n",
    "embedded_sequence = embeddings(example_sequence)\n",
    "\n",
    "print(f'Shape of embedded sequence: {embedded_sequence.shape}')\n",
    "print('\\nEmbedding for first character:')\n",
    "print(embedded_sequence[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings\n",
    "\n",
    "We can visualize the learned embeddings by projecting them to 2D using PCA. As we defined by default only 2D, we can directly visualise them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedding weights\n",
    "weights = embeddings.weight.detach().numpy()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(weights[:, 0], weights[:, 1], alpha=0.5)\n",
    "\n",
    "# Add character labels\n",
    "for i, char in enumerate(chars):\n",
    "    if char == '\\n': char = 'newline'  # Make newline visible\n",
    "    plt.annotate(char, (weights[i, 0], weights[i, 1]))\n",
    "\n",
    "plt.title('Character Embeddings 2D')\n",
    "plt.xlabel('First Component')\n",
    "plt.ylabel('Second Component')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Embedding function - CBOW at Character Level\n",
    "\n",
    "CBOW (Continuous Bag of Words) at a **character level** is a variant of the CBOW model used for learning vector representations of characters instead of words.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ How it Works:\n",
    "\n",
    "1. **Context Window**: Select a window of characters around a target character (e.g., 2 characters before and 2 after).\n",
    "   - For the word `\"hello\"`, if the target is `\"l\"` (the third character), the context might be `[\"h\", \"e\", \"l\", \"o\"]`.\n",
    "\n",
    "2. **Embedding**: Each context character is converted into a vector using an embedding layer.\n",
    "\n",
    "3. **Averaging**: The vectors of the context characters are averaged to form a single context vector (note no notion of relative position, hence **bag** of characters).\n",
    "\n",
    "4. **Prediction**: A softmax layer predicts the target character based on the context vector.\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "#### ðŸ”¹ Training process:\n",
    "\n",
    "1. **Simple NN**: We will define a simple NN architecture: Embeddings layer + output layer. \n",
    "\n",
    "2. **Loss function**: By considering CrossEntropy, we will calculate the loss function with the predicted characters vs the true center characters for each context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how we create training pairs\n",
    "def show_data_preparation_example():\n",
    "    # Take a small sample of text\n",
    "    sample_text = \"Hello, World!\"\n",
    "    print(f\"Sample text: {sample_text}\")\n",
    "    \n",
    "    # Convert to indices\n",
    "    indices = [char_to_idx[ch] for ch in sample_text]\n",
    "    context_size = 2\n",
    "    \n",
    "    print(\"\\nCreating context windows (context_size=2):\")\n",
    "    print(\"Center char | Context chars (2 before, 2 after)\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # Show some examples\n",
    "    for i in range(context_size, len(indices) - context_size):\n",
    "        center_char = sample_text[i]\n",
    "        context_chars = (\n",
    "            sample_text[i-context_size:i] + '*' +\n",
    "            sample_text[i+1:i+context_size+1]\n",
    "        )\n",
    "        \n",
    "        print(f\"    {center_char}     | {context_chars}\")\n",
    "        \n",
    "    print(\"\\nThese will be converted to tensor indices for training.\")\n",
    "    \n",
    "# Run the example\n",
    "show_data_preparation_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to indices tensor\n",
    "data = torch.tensor([char_to_idx[ch] for ch in text], dtype=torch.long)\n",
    "\n",
    "print(f\"Shape of data tensor: {data.shape}\")\n",
    "print(f\"First 20 characters as indices: {data[:20]}\")\n",
    "print(f\"As characters: {''.join([idx_to_char[idx.item()] for idx in data[:20]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "embedding_dim = 2\n",
    "context_size  = 2\n",
    "batch_size    = 1024\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "class SkipgramDataset(Dataset):\n",
    "    def __init__(self, data, context_size):\n",
    "        self.data = data\n",
    "        self.context_size = context_size\n",
    "        self.pairs = []\n",
    "        \n",
    "        # Create training pairs\n",
    "        for i in range(context_size, len(data) - context_size):\n",
    "            center = data[i]\n",
    "            context = torch.cat([\n",
    "                data[i-context_size:i],\n",
    "                data[i+1:i+context_size+1]\n",
    "            ])\n",
    "            self.pairs.append((center, context))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.pairs[idx]\n",
    "        return center, context\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SkipgramDataset(data, context_size)\n",
    "\n",
    "train_size = int(0.9*len(dataset))\n",
    "val_size   = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_dataloader           = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader      = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "rich.print(f'Number of training examples: {len(train_dataset):,}')\n",
    "rich.print(f'Number of validation examples: {len(val_dataset):,}')\n",
    "rich.print(f'Number of batches: {len(train_dataloader):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint = 0\n",
    "center    = idx_to_char[dataset[datapoint][0].item()]\n",
    "context   = ''\n",
    "for val in dataset[datapoint][1]:\n",
    "    context += idx_to_char[val.item()]\n",
    "\n",
    "print( f'Center: {center} | Context: {context[:2]}*{context[2:]}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simplest approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CharacterEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        # Embedding layer for context characters\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Output layer to predict center character\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, center_chars, context_chars):\n",
    "        # Get embeddings for context characters\n",
    "        context_embeds = self.embeddings(context_chars)  # [batch_size, context_size*2, embedding_dim]\n",
    "        \n",
    "        # Average context embeddings\n",
    "        context_embeds = context_embeds.mean(dim=1)  # [batch_size, embedding_dim]\n",
    "        \n",
    "        # Predict center character\n",
    "        logits = self.output(context_embeds)  # [batch_size, vocab_size]\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "model = CharacterEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Use AdamW optimizer with weight decay for better regularization\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Keep using CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding_epoch( model, train_dataloader, val_dataloader, optimizer, criterion):\n",
    "    \n",
    "    # -- Training loop --\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{n_epochs}') as pbar:\n",
    "        for center, context in pbar:\n",
    "            # Forward pass\n",
    "            logits = model(center, context)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(logits, center)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Update metrics\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "    # --- Validation loop ---\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for center, context in val_dataloader:\n",
    "            logits    = model(center, context)\n",
    "            loss      = criterion(logits, center)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    validation_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "    return train_loss, validation_loss\n",
    "\n",
    "def plot_losses( losses, val_losses ):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses, label='Train')\n",
    "    plt.plot(val_losses, label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 20\n",
    "losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, val_loss = train_embedding_epoch( model, train_dataloader, validation_dataloader, optimizer, criterion )\n",
    "\n",
    "    losses.append( train_loss )\n",
    "    val_losses.append( val_loss )\n",
    "    \n",
    "    # Plot the loss\n",
    "    plot_losses( losses, val_losses )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the trained embeddings (we use the main embeddings, not the context ones)\n",
    "trained_embeddings = model.embeddings.weight.detach().numpy()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(trained_embeddings[:, 0], trained_embeddings[:, 1], alpha=0.5)\n",
    "\n",
    "# Add character labels\n",
    "for i, char in enumerate(chars):\n",
    "    if char == '\\n': char = 'newline'  # Make newline visible\n",
    "    plt.annotate(char, (trained_embeddings[i, 0], trained_embeddings[i, 1]))\n",
    "\n",
    "plt.title('Character Embeddings 2D')\n",
    "plt.xlabel('First Component')\n",
    "plt.ylabel('Second Component')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Add a hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Add batch normalization for better training stability\n",
    "        self.batch_norm = nn.BatchNorm1d(embedding_dim)\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)  # Add dropout for regularization\n",
    "        )\n",
    "        \n",
    "        # Output layer to predict center character\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, center_chars, context_chars):\n",
    "        # Get embeddings for context characters\n",
    "        context_embeds = self.embeddings(context_chars)  # [batch_size, context_size*2, embedding_dim]\n",
    "        \n",
    "        # Average context embeddings\n",
    "        context_embeds = context_embeds.mean(dim=1)  # [batch_size, embedding_dim]\n",
    "        context_embeds = self.batch_norm(context_embeds)\n",
    "        \n",
    "        # Pass through hidden layers\n",
    "        hidden = self.hidden(context_embeds)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Predict center character\n",
    "        logits = self.output(hidden)  # [batch_size, vocab_size]\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "model = CharacterEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Use AdamW optimizer with weight decay for better regularization\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Keep using CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "n_epochs = 50\n",
    "losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, val_loss = train_embedding_epoch( model, train_dataloader, validation_dataloader, optimizer, criterion )\n",
    "\n",
    "    losses.append( train_loss )\n",
    "    val_losses.append( val_loss )\n",
    "    \n",
    "    # Plot the losses\n",
    "    plot_losses( losses, val_losses )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"embeddings_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model_path = 'embeddings_model.pth'\n",
    "embeddings_model_url  = 'https://raw.githubusercontent.com/marc-olm/genai101/main/notebooks/embeddings_model.pth'\n",
    "\n",
    "!curl -L -o  {embeddings_model_path} {embeddings_model_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the state dict\n",
    "model.load_state_dict(torch.load(embeddings_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viz new embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the trained embeddings (we use the main embeddings, not the context ones)\n",
    "trained_embeddings = model.embeddings.weight.detach().numpy()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(trained_embeddings[:, 0], trained_embeddings[:, 1], alpha=0.5)\n",
    "\n",
    "# Add character labels\n",
    "for i, char in enumerate(chars):\n",
    "    if char == '\\n': char = 'newline'  # Make newline visible\n",
    "    plt.annotate(char, (trained_embeddings[i, 0], trained_embeddings[i, 1]))\n",
    "\n",
    "plt.title('Character Embeddings 2D')\n",
    "plt.xlabel('First Component')\n",
    "plt.ylabel('Second Component')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, train a model to produce new characters! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-Based Character-Level Language Model\n",
    "\n",
    "This model (`CharacterLM`) is a **character-level language model**. It learns to predict the **next character** in a sequence using an LSTM (Long Short-Term Memory) network, using the next character as the target. \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ How it Works:\n",
    "\n",
    "1. **Input**: A sequence of characters, represented as integer indices. Next character predictor, based on the previous context. \n",
    "   \n",
    "2. **Embedding Layer**: Converts each character index into a dense vector of size `embedding_dim`.\n",
    "\n",
    "3. **LSTM Layer**: Processes the sequence of embeddings, capturing contextual information and dependencies between characters.\n",
    "\n",
    "4. **Fully Connected Layer**: Output layer, projects the LSTM outputs to the vocabulary size, producing a distribution over possible next characters at each position.\n",
    "\n",
    "5. **Loss Function**: Cross-entropy is used to compare predicted character probabilities against the actual next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert entire text to indices\n",
    "data = torch.tensor([char_to_idx[ch] for ch in text], dtype=torch.long)\n",
    "\n",
    "# Create sequences of context_length characters\n",
    "context_length = 4\n",
    "x = torch.stack([data[i:i+context_length] for i in range(len(data)-context_length)])\n",
    "y = data[context_length:]\n",
    "\n",
    "print(f'Shape of input sequences: {x.shape}')\n",
    "print(f'Shape of target values: {y.shape}')\n",
    "\n",
    "# Show an example\n",
    "idx = 0  # First sequence\n",
    "context = ''.join([idx_to_char[int(i)] for i in x[idx]])\n",
    "next_char = idx_to_char[int(y[idx])]\n",
    "print(f'\\nExample:')\n",
    "print(f'Context: \"{context}\"')\n",
    "print(f'Next character: \"{next_char}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CharacterLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length)\n",
    "        embedded = self.embeddings(x)  # (batch_size, sequence_length, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(embedded)  # (batch_size, sequence_length, hidden_dim)\n",
    "        logits = self.fc(lstm_out)  # (batch_size, sequence_length, vocab_size)\n",
    "        return logits\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim  = 16\n",
    "hidden_dim     = 32\n",
    "context_length = 16\n",
    "batch_size     = 128\n",
    "\n",
    "# Initialize model\n",
    "model = CharacterLM(vocab_size, embedding_dim, hidden_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, context_length):\n",
    "        self.data = data\n",
    "        self.context_length = context_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.context_length - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get input sequence and target\n",
    "        x = self.data[idx:idx + self.context_length]\n",
    "        y = self.data[idx + 1:idx + self.context_length + 1]\n",
    "        return x, y\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TextDataset(data, context_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Number of training examples: {len(dataset):,}')\n",
    "print(f'Number of batches: {len(dataloader):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in tqdm(dataloader, desc='Training'):\n",
    "        # Forward pass\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_epochs = 20\n",
    "\n",
    "# Train the model\n",
    "losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    loss = train_epoch(model, dataloader, optimizer, criterion)\n",
    "    losses.append(loss)\n",
    "\n",
    "    # Plot the loss\n",
    "    clear_output(wait=True)\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Loss: {loss:.4f}')\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses, label='Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"character_generator_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_generator_model_path = 'character_generator_model.pth'\n",
    "character_generator_model_url  = 'https://raw.githubusercontent.com/marc-olm/genai101/main/notebooks/character_generator_model.pth'\n",
    "\n",
    "!curl -L -o {character_generator_model_path} {character_generator_model_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the state dict\n",
    "model.load_state_dict(torch.load(character_generator_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(model, start_text, max_length=200, temperature=1.0):\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert start text to indices\n",
    "    context = torch.tensor([char_to_idx[ch] for ch in start_text])\n",
    "    generated_text = start_text\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Get model predictions\n",
    "        x = context[-context_length:].unsqueeze(0)  # Add batch dimension\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits[0, -1] / temperature, dim=0)\n",
    "        \n",
    "        # Sample next character\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "        next_char = idx_to_char[next_idx]\n",
    "        \n",
    "        # Update context and generated text\n",
    "        generated_text += next_char\n",
    "        context = torch.cat([context, torch.tensor([next_idx])])\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some text\n",
    "prompt = \"To be, or not to be\"\n",
    "\n",
    "rich.print(f'\\nPrompt: \"{prompt}\"')\n",
    "rich.print('\\n\\nCompletions:')\n",
    "rich.print(generate_text(model, prompt, max_length=200, temperature=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some text\n",
    "prompt = \"Oh Romeo, Romeo, \"\n",
    "\n",
    "rich.print(f'\\nPrompt: \"{prompt}\"')\n",
    "rich.print('\\n\\nCompletions:')\n",
    "rich.print(generate_text(model, prompt, max_length=200, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Shakesperian name! \n",
    "# Since the learned representation contains uppercase for character names \n",
    "prompt = \"MARC\"\n",
    "\n",
    "rich.print(f'\\nPrompt: \"{prompt}\"')\n",
    "\n",
    "completion = generate_text(model, prompt, max_length=10, temperature=0.5)\n",
    "# rich.print(completion)\n",
    "rich.print( f'Shakespearean name: \"{completion.split(\":\")[0]}\"' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai101)",
   "language": "python",
   "name": "genai101-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
